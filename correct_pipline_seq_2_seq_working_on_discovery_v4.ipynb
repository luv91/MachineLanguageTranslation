{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1693282-acfe-4295-a954-ce47fda9ab4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import pickle\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, causal = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # assume d_v = d_k\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.key = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k*n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k*n_heads)\n",
    "        \n",
    "        # final linear layer\n",
    "        self.fc = nn.Linear(d_k* n_heads, d_model)\n",
    "        \n",
    "        # causal mask\n",
    "        # make it so that diagonal is 0 too.. \n",
    "        # this way we dont have to shift the inputs to make targets\n",
    "        \n",
    "        self.causal = causal\n",
    "        \n",
    "        # causal mask if causal has been set to true. \n",
    "        if causal:\n",
    "            \n",
    "            cm = torch.tril(torch.ones(max_len, max_len))\n",
    "            \n",
    "            self.register_buffer(\n",
    "                \"causal_mask\",\n",
    "                cm.view(1,1, max_len, max_len)\n",
    "            )\n",
    "            \n",
    "    def forward(self, q,k,v, pad_mask = None):\n",
    "        \n",
    "        q = self.query(q)  # N X T X (hd_k)\n",
    "        k = self.key(k)  # N X T X (hd_k)\n",
    "        v = self.value(v)  # N X T X (hd_k)\n",
    "        \n",
    "        # in seq-2-seq, it is possible to apply attention where we want to know, \n",
    "        # which decoder output should pay attention to which encoder input. \n",
    "        # this is a cross-attention part, wehre encoder is connected to the decoder. \n",
    "        # k and v comes from the encoder while q (query ) comes from the decoder. \n",
    "        # so input to this layer comes from encoder input (k), while outptu shape will be q (since output comes from query in decoder)\n",
    "        N = q.shape[0]\n",
    "        T_output = q.shape[1]\n",
    "        T_input = k.shape[1]\n",
    "        \n",
    "        #change the shape to:\n",
    "        # (N,T,h,d_k) --> (N,h,T,d_k)\n",
    "        \n",
    "        # in order for amtrix multiply to work properly\n",
    "        \n",
    "        q = q.view(N, T_output, self.n_heads, self.d_k).transpose(1,2)\n",
    "        k = k.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
    "        v = v.view(N, T_input, self.n_heads, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # compute attention weights\n",
    "        # (N,h,T, d_k) x(N,h,d_k, T) --> (N,h,T,T)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2,-1) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if pad_mask is not None:\n",
    "            \n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                pad_mask[:,None,None,:] == 0, float('-inf'))\n",
    "            \n",
    "        # what changes is how we apply thhe causal mask.. \n",
    "        if self.causal:\n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                self.causal_mask[:,:,:T_output, :T_input] == 0, float('-inf'))\n",
    "            \n",
    "        attn_weights = F.softmax(attn_scores, dim = -1)\n",
    "        \n",
    "        \n",
    "        # compute attention-weighted values\n",
    "        # (N,h,T,T) x (N,h,T,d_k) --> (N,h, T, d_k)\n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        # reshape it back before final linear layer\n",
    "        A = A.transpose(1,2)  # (N,T,h,d_k)\n",
    "        \n",
    "        A = A.contiguous().view(N,T_output, self.d_k*self.n_heads)   # (N, T, h*d_k)\n",
    "        \n",
    "        # projection \n",
    "        return self.fc(A)\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_model, n_heads, max_len,dropout_prob = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(d_model*4, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        \n",
    "        x = self.ln1(x+self.mha(x,x,x,pad_mask))\n",
    "        x = self.ln2(x+ self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_k, d_model, n_heads, max_len,dropout_prob = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.mha1 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = True)  # this layer is cross-attention layer\n",
    "        self.mha2 = MultiHeadAttention(d_k, d_model, n_heads, max_len, causal = False)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*4),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(d_model*4, d_model),\n",
    "            nn.Dropout(dropout_prob),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        \n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
    "        \n",
    "        x = self.ln1(\n",
    "            dec_input + self.mha1(dec_input, dec_input, dec_input, dec_mask)\n",
    "        )\n",
    "        \n",
    "        # multi-head attention including encoder output\n",
    "        \n",
    "        x = self.ln2(x + self.mha2(x, enc_output, enc_output, enc_mask))\n",
    "        \n",
    "        x = self.ln3(x+self.ann(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, max_len = 2048, dropout_prob = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout_prob)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2)\n",
    "        div_term = torch.exp(exp_term* (-math.log(10000.0)/ d_model))\n",
    "\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position* div_term)\n",
    "        pe[0,:, 1::2] = torch.cos(position* div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x.shape: N xT X D\n",
    "        x = x + self.pe[:,:x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, \n",
    "                 n_heads, n_layers, dropout_prob):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        \n",
    "        transformer_blocks = [\n",
    "            \n",
    "            EncoderBlock(d_k, \n",
    "                         d_model, \n",
    "                         n_heads, \n",
    "                         max_len,\n",
    "                         dropout_prob) for _ in range(n_layers)]\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, pad_mask = None):\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            \n",
    "            x = block(x, pad_mask)\n",
    "            \n",
    "        x = self.ln(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, \n",
    "                 n_heads, n_layers, dropout_prob):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        \n",
    "        transformer_blocks = [\n",
    "            \n",
    "            DecoderBlock(d_k, \n",
    "                         d_model, \n",
    "                         n_heads, \n",
    "                         max_len,\n",
    "                         dropout_prob) for _ in range(n_layers)]\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, enc_output, dec_input, enc_mask = None, dec_mask = None):\n",
    "        \n",
    "        x = self.embedding(dec_input)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(enc_output, x, enc_mask, dec_mask)\n",
    "            \n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)  # many - to -many\n",
    "\n",
    "        return x\n",
    "            \n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self,enc_input, dec_input, enc_mask, dec_mask):\n",
    "        \n",
    "        enc_output = self.encoder(enc_input, enc_mask)\n",
    "        dec_output = self.decoder(enc_output, dec_input, enc_mask, dec_mask)\n",
    "        \n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f57894-b795-46c3-b43b-9b2f17228794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, model_checkpoint='Helsinki-NLP/opus-mt-en-es', \n",
    "                 max_input_length=128, max_target_length=128):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        self.tokenizer.add_special_tokens({\"cls_token\": \"<s>\"})\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        # df = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "        # df = df.iloc[:30000]\n",
    "        # df.columns = ['en', 'es']\n",
    "        # df.to_csv('spa.csv', index=None)\n",
    "        raw_dataset = load_dataset('csv', data_files='spa.csv')\n",
    "        split = raw_dataset['train'].train_test_split(test_size=0.3, seed=42)\n",
    "        tokenized_datasets = split.map(\n",
    "            self.preprocess_function, batched=True,\n",
    "            remove_columns=split[\"train\"].column_names,\n",
    "        )\n",
    "        return tokenized_datasets\n",
    "\n",
    "    def preprocess_function(self, batch):\n",
    "        model_inputs = self.tokenizer(\n",
    "            batch['en'], max_length=self.max_input_length, truncation=True)\n",
    "        labels = self.tokenizer(\n",
    "            batch['es'], max_length=self.max_target_length, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    def prepare_dataloader(self, tokenized_datasets, batch_size=32):\n",
    "        data_collator = DataCollatorForSeq2Seq(self.tokenizer)\n",
    "        train_loader = DataLoader(\n",
    "            tokenized_datasets[\"train\"],\n",
    "            shuffle=True,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            tokenized_datasets[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=data_collator\n",
    "        )\n",
    "        return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0b8c57-2dc4-43e9-b41a-323b14036afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device, tokenizer, train_loader, valid_loader, key):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        # Add tensorboard writer to Trainer class\n",
    "        # self.writer = SummaryWriter()\n",
    "        self.writer = SummaryWriter(log_dir=f'runs/{key}')\n",
    "\n",
    "    \n",
    "    \n",
    "    # def print_number_of_trainable_model_parameters(self):\n",
    "    #     trainable_model_params = 0\n",
    "    #     all_model_params = 0\n",
    "    #     for _, param in self.model.named_parameters():\n",
    "    #         all_model_params += param.numel()\n",
    "    #         if param.requires_grad:\n",
    "    #             trainable_model_params += param.numel()\n",
    "    #     # print(f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")\n",
    "    #     return (f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        \n",
    "        def get_total_params(module: torch.nn.Module):\n",
    "            total_params = 0\n",
    "            for param in module.parameters():\n",
    "                total_params += param.numel()\n",
    "            return total_params\n",
    "\n",
    "        print('Total parameters in model: {:,}'.format(get_total_params(self.model)))\n",
    "        \n",
    "        train_losses = np.zeros(epochs)\n",
    "        validation_losses = np.zeros(epochs)\n",
    "        train_perplexity_list = np.zeros(epochs)\n",
    "        validation_perplexity_list = np.zeros(epochs)\n",
    "        # print(\"print_number_of_trainable_model_parameters\",\n",
    "        #       self.print_number_of_trainable_model_parameters()) \n",
    "        for it in range(epochs):\n",
    "            self.model.train()\n",
    "            t0 = datetime.now()\n",
    "            train_loss = [] \n",
    "            train_loss_scalar, train_correct_scalar, total_train_samples_scalar = 0, 0, 0\n",
    "            for batch in self.train_loader:\n",
    "                batch = {k:v.to(self.device) for k,v in batch.items()}\n",
    "                self.optimizer.zero_grad()\n",
    "                enc_input = batch['input_ids']\n",
    "                enc_mask = batch['attention_mask']\n",
    "                targets = batch['labels']\n",
    "                dec_input, dec_mask = self.prepare_decoder_inputs(targets)\n",
    "                outputs = self.model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "                loss = self.criterion(outputs.transpose(2,1), targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_loss.append(loss.item())\n",
    "                \n",
    "                train_loss_scalar +=loss.item()\n",
    "                _,predicted = torch.max(outputs, dim = 2)\n",
    "                train_correct_scalar += (predicted == targets).sum().item()\n",
    "                total_train_samples_scalar += targets.ne(self.tokenizer.pad_token_id).sum().item()\n",
    "                \n",
    "                \n",
    "            train_loss = np.mean(train_loss)\n",
    "            # test_loss = self.evaluate()\n",
    "            avg_train_loss_scalar = train_loss_scalar / len(self.train_loader)\n",
    "            train_accuracy_scalar = train_correct_scalar / total_train_samples_scalar\n",
    "            train_perplexity_scalar = np.exp(avg_train_loss_scalar)\n",
    "            \n",
    "            \n",
    "            avg_valid_loss_scalar, valid_accuracy_scalar, valid_perplexity_scalar,test_loss = self.evaluate()\n",
    "            \n",
    "            dt = datetime.now() - t0\n",
    "            dt_seconds = dt.total_seconds()  # convert duration to seconds\n",
    "            \n",
    "             # Log the metrics to tensorboard\n",
    "            self.writer.add_scalar('Train Loss', avg_train_loss_scalar, it)\n",
    "            self.writer.add_scalar('Train Accuracy', train_accuracy_scalar, it)\n",
    "            self.writer.add_scalar('Train Perplexity', train_perplexity_scalar, it)\n",
    "            self.writer.add_scalar('Validation Loss', avg_valid_loss_scalar, it)\n",
    "            self.writer.add_scalar('Validation Accuracy', valid_accuracy_scalar, it)\n",
    "            self.writer.add_scalar('Validation Perplexity', valid_perplexity_scalar, it)\n",
    "            self.writer.add_scalar('Epoch Duration', dt_seconds, it)\n",
    "            \n",
    "            \n",
    "            train_losses[it] = train_loss\n",
    "            validation_losses[it] = test_loss\n",
    "            \n",
    "            train_perplexity_list[it], validation_perplexity_list[it] =train_perplexity_scalar, valid_perplexity_scalar\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Log the duration of this epoch to tensorboard\n",
    "            \n",
    "            \n",
    "            print(f'Epoch {it+1}/{epochs}, Train Loss: {avg_train_loss_scalar:.4f}, \\\n",
    "                  Train Accuracy: {train_accuracy_scalar:.4f}, \\\n",
    "                  Train Perplexity: {train_perplexity_scalar:.4f}, \\\n",
    "                  Validation Loss: {avg_valid_loss_scalar:.4f}, \\\n",
    "                  Validation Accuracy: {valid_accuracy_scalar:.4f}, \\\n",
    "                  Validation Perplexity: {valid_perplexity_scalar:.4f}')\n",
    "            \n",
    "            # print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss: .4f}, Test Loss: {test_loss: .4f}, Duration: {dt}')\n",
    "        self.writer.close()  # Close the writer after logging all losses\n",
    "        return train_losses, validation_losses, train_perplexity_list, validation_perplexity_list\n",
    "\n",
    "    def prepare_decoder_inputs(self, targets):\n",
    "        dec_input = targets.clone().detach()\n",
    "        dec_input = torch.roll(dec_input, shifts = 1, dims = 1)\n",
    "        dec_input[:,0] = 65001\n",
    "        dec_input = dec_input.masked_fill(dec_input==-100, self.tokenizer.pad_token_id )\n",
    "        dec_mask = torch.ones_like(dec_input)\n",
    "        dec_mask = dec_mask.masked_fill(dec_input == self.tokenizer.pad_token_id,0)\n",
    "        return dec_input, dec_mask\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        test_loss = []\n",
    "        valid_loss, valid_correct, total_valid_samples = 0, 0, 0\n",
    "        for batch in self.valid_loader:\n",
    "            batch = {k:v.to(self.device) for k,v in batch.items()}\n",
    "            enc_input = batch['input_ids']\n",
    "            enc_mask = batch['attention_mask']\n",
    "            targets = batch['labels']\n",
    "            dec_input, dec_mask = self.prepare_decoder_inputs(targets)\n",
    "            outputs = self.model(enc_input, dec_input, enc_mask, dec_mask)\n",
    "            loss = self.criterion(outputs.transpose(2,1), targets)\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=2)\n",
    "            valid_correct += (predicted == targets).sum().item()\n",
    "            total_valid_samples += targets.ne(self.tokenizer.pad_token_id).sum().item()\n",
    "        avg_valid_loss = valid_loss / len(self.valid_loader)\n",
    "        valid_accuracy = valid_correct / total_valid_samples\n",
    "        valid_perplexity = np.exp(avg_valid_loss)\n",
    "        # return \n",
    "            \n",
    "        return avg_valid_loss, valid_accuracy, valid_perplexity,np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c3ff30-89b0-45ef-b482-dbb00e225860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/verma.lu/.cache/huggingface/datasets/csv/default-b2684800da7028a4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a36ffb50914124b7350503de201c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/verma.lu/.cache/huggingface/datasets/csv/default-b2684800da7028a4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-b7b9a29e44f1c96c.arrow and /home/verma.lu/.cache/huggingface/datasets/csv/default-b2684800da7028a4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-1c7564918c430525.arrow\n",
      "Loading cached processed dataset at /home/verma.lu/.cache/huggingface/datasets/csv/default-b2684800da7028a4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-67f9299eab000569.arrow\n",
      "Loading cached processed dataset at /home/verma.lu/.cache/huggingface/datasets/csv/default-b2684800da7028a4/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d/cache-9760a8a79c5292f6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with configuration: 512_16_64_4_2_0.5\n",
      "Total parameters in model: 12,779,114\n",
      "Epoch 1/20, Train Loss: 5.5146,                   Train Accuracy: 0.1273,                   Train Perplexity: 248.2836,                   Validation Loss: 4.8868,                   Validation Accuracy: 0.1726,                   Validation Perplexity: 132.5273\n",
      "Epoch 2/20, Train Loss: 4.5325,                   Train Accuracy: 0.1733,                   Train Perplexity: 92.9928,                   Validation Loss: 4.6304,                   Validation Accuracy: 0.1858,                   Validation Perplexity: 102.5584\n",
      "Epoch 3/20, Train Loss: 4.3282,                   Train Accuracy: 0.1878,                   Train Perplexity: 75.8047,                   Validation Loss: 4.3410,                   Validation Accuracy: 0.2027,                   Validation Perplexity: 76.7868\n",
      "Epoch 4/20, Train Loss: 4.1907,                   Train Accuracy: 0.1962,                   Train Perplexity: 66.0685,                   Validation Loss: 4.1281,                   Validation Accuracy: 0.2146,                   Validation Perplexity: 62.0609\n",
      "Epoch 5/20, Train Loss: 4.0632,                   Train Accuracy: 0.2038,                   Train Perplexity: 58.1595,                   Validation Loss: 4.0184,                   Validation Accuracy: 0.2226,                   Validation Perplexity: 55.6105\n",
      "Epoch 6/20, Train Loss: 3.9446,                   Train Accuracy: 0.2113,                   Train Perplexity: 51.6541,                   Validation Loss: 3.8448,                   Validation Accuracy: 0.2313,                   Validation Perplexity: 46.7488\n",
      "Epoch 7/20, Train Loss: 3.8442,                   Train Accuracy: 0.2169,                   Train Perplexity: 46.7207,                   Validation Loss: 3.7250,                   Validation Accuracy: 0.2384,                   Validation Perplexity: 41.4704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Create an instance of the data handler\n",
    "    data_handler = DataHandler()\n",
    "\n",
    "    # Load the dataset from the file and get the tokenized datasets\n",
    "    tokenized_datasets = data_handler.load_dataset('spa.csv')\n",
    "\n",
    "    # Prepare the data loaders\n",
    "    train_loader, valid_loader = data_handler.prepare_dataloader(tokenized_datasets)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    max_lens = [512]\n",
    "    model_sizes = [64, 128, 256, 512, 1024]\n",
    "    d_ks =[16]\n",
    "    # model_sizes =[64]\n",
    "    n_heads =[4, 8, 16, 32]\n",
    "    n_layers =[2, 4, 8]\n",
    "    dropout_prob = [0.3, 0.4, 0.5]\n",
    "    epochs = 100\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for max_len in max_lens:\n",
    "        # for d_k in d_ks:\n",
    "        for model_size in model_sizes:\n",
    "            for n_head in n_heads:\n",
    "                for n_layer in n_layers:\n",
    "                    for dropout in dropout_prob:\n",
    "\n",
    "                        key = '_'.join(str(x) for x in (max_len, d_k, model_size,n_head,n_layer,dropout))\n",
    "                        print(\"Running experiment with configuration:\", key)\n",
    "\n",
    "\n",
    "\n",
    "                        encoder = Encoder(vocab_size=data_handler.tokenizer.vocab_size+1,\n",
    "                                        max_len = max_len,\n",
    "                                        d_k = int(model_size//n_head),\n",
    "                                        d_model = model_size,\n",
    "                                        n_heads = n_head,\n",
    "                                        n_layers = n_layer,\n",
    "                                        dropout_prob = dropout)\n",
    "                        encoder.to(device)\n",
    "\n",
    "                        decoder = Decoder(vocab_size=data_handler.tokenizer.vocab_size+1,\n",
    "                                        max_len =max_len,\n",
    "                                        d_k=int(model_size//n_head),\n",
    "                                        d_model=model_size,\n",
    "                                        n_heads=n_head,\n",
    "                                        n_layers=n_layer,\n",
    "                                        dropout_prob=dropout)\n",
    "                        decoder.to(device)\n",
    "\n",
    "                        transformer = Transformer(encoder, decoder)\n",
    "\n",
    "                        criterion = torch.nn.CrossEntropyLoss(ignore_index = -100)\n",
    "                        optimizer = torch.optim.Adam(transformer.parameters())\n",
    "\n",
    "                        trainer = Trainer(transformer, criterion, optimizer, device, data_handler.tokenizer, train_loader, valid_loader, key)\n",
    "                        train_losses, validation_losses, train_perplexity_list, validation_perplexity_list = trainer.train(epochs=epochs)\n",
    "\n",
    "                        results[key] = {\n",
    "                            \"train_losses_list\":train_losses,\n",
    "                            \"validation_losses_list\":validation_losses,\n",
    "                            \"model_size\": model_size,\n",
    "                            \"d_k\": d_k,\n",
    "                            \"max_len\": max_len,\n",
    "                            \"n_head\": n_head,\n",
    "                            \"n_layer\": n_layer,\n",
    "                            \"dropout\": dropout,\n",
    "                        }\n",
    "\n",
    "                            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba997c5-9be4-4fb5-9e34-2b3760354917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb794c-d655-430d-b6ba-18d7502ee7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d94088-d93a-43ec-8e3f-337b0e4426ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
