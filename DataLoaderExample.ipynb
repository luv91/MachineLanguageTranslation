{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I'm fine, thank you.\",\n",
    "    \"This is an example of different length sentences.\",\n",
    "    \"NewYork, what the hell?\",\n",
    "    \"How is everybody doing?\",\n",
    "    \"I wanna dance with somebody\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101, 7592, 1010, 2129, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1045, 1005, 1049, 2986, 1010, 4067, 2017, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 2023, 2003, 2019, 2742, 1997, 2367, 3091, 11746, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 2047, 7677, 8024, 1010, 2054, 1996, 3109, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 2129, 2003, 7955, 2725, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]},\n",
       " {'input_ids': [101, 1045, 10587, 3153, 2007, 8307, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode sequences\n",
    "encoded_sequences = [tokenizer.encode_plus(sequence, truncation=True, padding=False) for sequence in sequences]\n",
    "encoded_sequences # bert based encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0,\n",
      "             0],\n",
      "        [  101,  1045,  1005,  1049,  2986,  1010,  4067,  2017,  1012,   102,\n",
      "             0],\n",
      "        [  101,  2023,  2003,  2019,  2742,  1997,  2367,  3091, 11746,  1012,\n",
      "           102],\n",
      "        [  101,  2047,  7677,  8024,  1010,  2054,  1996,  3109,  1029,   102,\n",
      "             0],\n",
      "        [  101,  2129,  2003,  7955,  2725,  1029,   102,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1045, 10587,  3153,  2007,  8307,   102,     0,     0,     0,\n",
      "             0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n",
      "torch.Size([6, 11])\n",
      "torch.Size([6, 11])\n",
      "torch.Size([6, 11])\n"
     ]
    }
   ],
   "source": [
    "# Collate the batch of data. \n",
    "collated_batch = data_collator(encoded_sequences)\n",
    "\n",
    "print(collated_batch)\n",
    "print(collated_batch['input_ids'].shape)\n",
    "print(collated_batch['token_type_ids'].shape)\n",
    "print(collated_batch['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2840280dc50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(encoded_sequences, batch_size=batch_size, collate_fn=data_collator)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_batch {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102,    0,    0],\n",
      "        [ 101, 1045, 1005, 1049, 2986, 1010, 4067, 2017, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# dataloader is a generator\n",
    "# Convert the first batch to a list and check the keys\n",
    "first_batch = next(iter(dataloader))\n",
    "print(\"first_batch\",first_batch)\n",
    "print(list(first_batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: torch.Size([2, 10])\n",
      "Attention mask: torch.Size([2, 10])\n",
      "---\n",
      "Input IDs: torch.Size([2, 11])\n",
      "Attention mask: torch.Size([2, 11])\n",
      "---\n",
      "Input IDs: torch.Size([2, 7])\n",
      "Attention mask: torch.Size([2, 7])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Iterate over batches\n",
    "for batch in dataloader:\n",
    "    print(\"Input IDs:\", batch['input_ids'].size())\n",
    "    print(\"Attention mask:\", batch['attention_mask'].size())\n",
    "    print(\"---\")\n",
    "    \n",
    "    # Input IDs: torch.Size([2, 10]) ==> because the batch size is 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
